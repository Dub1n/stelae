# MCP Connector Integration and Troubleshooting in ChatGPT Developer Mode

## Attempting to Invoke an MCP Tool in Deep Research Mode

We first attempted to call the **list_allowed_directories** tool from the custom **"stelae"** MCP connector within a Deep Research session. This tool (as defined in the connector's manifest) should return a list of directories the server can access. However, our direct invocation did not succeed - no output was returned, and the call seemingly had no effect. In other words, the connector's tool was **not accessible** through the Deep Research interface. This indicates that the custom MCP connector was not mounted or recognized in this chat session, or that its tools were being filtered out by the current mode's restrictions.

To confirm, we checked whether any **MCP connectors** were active in the session. The system's list of enabled data sources showed only the GitHub connector, with no mention of "stelae" or other custom connectors. This suggests the stelae connector wasn't successfully loaded. We also tried alternate call strategies (for example, using an internal API tool function, or addressing the tool via a path like /stelae/list_allowed_directories), but these similarly failed - essentially, ChatGPT did not recognize the existence of the list_allowed_directories capability at all. The failure point occurred at the very invocation step: the assistant did not execute the tool call and provided no directory listing. The absence of any error message in the chat UI further implies the connector's tools were never registered in this environment.

At this stage, the likely cause is that **Deep Research mode does not expose custom MCP tools beyond the basic search/fetch operations** (and our connector may not even have been initialized without those). In Deep Research (and regular Chat mode), ChatGPT will typically only use connectors for retrieving information (e.g. searching documents or fetching content), not for arbitrary custom actions. Indeed, OpenAI's documentation explicitly states that _"to work with ChatGPT Connectors or deep research (in ChatGPT or via API), your MCP server must implement two tools - search and fetch."_ [1]
These are the minimum required actions for an MCP to be considered compliant by ChatGPT. If a connector doesn't expose a tool named exactly search (for querying data) and fetch (for retrieving a specific result by ID), the platform will **refuse to use that connector**. This is evidenced by an error one developer encountered: _"This MCP server doesn't implement our specification:_ _search_ _action not found."_ [2]
In that case, the connector (for Monday.com) had custom-named actions but lacked a proper search, causing ChatGPT to abort the connection [2]. It's likely that our stelae connector faced a similar issue - its manifest lists a grep tool (for code search) instead of a standard search, which may have prevented ChatGPT from accepting it in Deep Research mode. In contrast, another AI agent (Claude) was more lenient and "sees the full toolset… without complaint," but ChatGPT strictly required the search tool [2].

Because our initial tool call failed silently, we shifted into diagnosing why **Deep Research mode** couldn't invoke the custom tools. We needed to determine if the connector was loaded at all, and if so, why its tools were inaccessible. The underlying cause became clearer by examining how ChatGPT integrates MCP connectors under different modes and what configurations it expects. Below, we break down the key factors:

## How ChatGPT Loads MCP Connectors in Different Modes

ChatGPT supports custom connectors in several modes, each with different capabilities:

- **Standard Chat Mode (with Connectors)** - In regular usage (ChatGPT Plus/Enterprise), users can connect to custom MCP servers but **only read-type operations are supported** here. The assistant can perform **searches and fetches** via the connector to retrieve information, and will then incorporate those results into answers (often with citations) [1]. This mode is limited to read-only interactions for security. The custom connector must provide the required search/fetch API for ChatGPT to use it in normal chat [1]. If those are missing or misnamed, ChatGPT will refuse to use the connector (as we saw with the "search action not found" error) [2]. Even if the connector defines other tools, the ChatGPT UI will **strip them out** in this mode - essentially ignoring any capabilities beyond search and fetch. This is by design: during normal chats, ChatGPT isn't allowed to execute arbitrary actions on user systems, only to retrieve information.
- **Deep Research Mode** - This is a special mode intended for _in-depth analysis across multiple sources_, producing fully cited reports [3]. Deep Research can utilize connectors similarly to chat, but in an **autonomous, multi-step fashion**. When enabled, ChatGPT will leverage connected sources (and the web) to gather information, reason over it, and then produce a comprehensive answer with citations [3]. Importantly, Deep Research still treats custom connectors as **information providers**: it will call the connector's search tool to find relevant documents, then call fetch to retrieve their content, and finally compose an answer. **Only read-only tools are used**, and the same requirement for a search and fetch pairing applies. In a community discussion, developers noted their MCP server logs showed ChatGPT calling search and returning results, but Deep Research failed to actually use those results in the final report - indicating possible format mismatches or other issues in the handoff. In our case, since the stelae connector's search capability wasn't registered (due to naming or config issues), Deep Research simply didn't involve that connector at all. The end result was that our tool calls (like list_allowed_directories) were never even attempted by the agent.
- **Developer Mode (Full MCP Tool Support)** - This mode, recently introduced, is where ChatGPT truly unlocks **all tools** exposed by a custom connector. In Developer Mode, _write actions and custom workflows are allowed_, not just search/fetch [4]. This means once Developer Mode is enabled and your connector is added, ChatGPT can call tools that modify external state or perform complex actions (e.g. updating a database, executing code, creating files) [4]. Our stelae connector includes tools like directory_tree, edit_file, execute_command, etc., which would fall under this category. However, these will **only be accessible in Developer Mode**, not in Deep Research or normal chat [1]. Developer Mode is currently available to Plus/Pro users (and certain enterprise tiers) and must be manually activated in ChatGPT's settings [5]. In the UI, one would go to **Settings → Connectors → Advanced → Developer Mode** to turn it on [5]. After that, you can import your custom connector by providing its base URL (which points to its .well-known/mcp/manifest.json endpoint). If the connector passes the initial validation (again, it **must** list search and fetch in its manifest [2]), it will appear as an available tool source in the chat interface.
- **Agent/API Integration** - Beyond the ChatGPT web UI, OpenAI provides an API/SDK for using connectors programmatically. This is part of the "OpenAI Agents" system. Developers can register remote MCP servers as tools in their own agent implementations or via the API. For example, the OpenAI **JavaScript agents SDK** offers a hostedMcpTool utility: you supply the server's label and URL, and the OpenAI backend will call that remote MCP endpoint when the agent needs to use its tools [6]. In this context, the model (e.g. GPT-4 via the API) can directly invoke the connector's tools as function calls, with the API orchestrating the HTTP/SSE connection in the background [6]. The API expects the same manifest schema and tool definitions - so the search/fetch requirement and JSON schemas still apply. In essence, the ChatGPT developer mode is a convenient wrapper around this capability for the web interface, whereas via the API a developer could achieve similar results by writing an agent that includes the remote tool. (For instance, using the **OpenAI Python SDK or JS SDK**, one could create an agent that _always uses_ the MCP tools to answer questions [6].)

**In summary,** ChatGPT distinguishes between _read-only data retrieval_ (permitted in normal and deep research modes) and _full action execution_ (permitted only in dev mode or via the API/agents). Our scenario ran into the limitations of Deep Research mode: it wouldn't list or execute list_allowed_directories because that tool isn't part of the basic search/fetch schema and likely wasn't recognized at all outside Developer Mode.

## Requirements for MCP Tool Exposure in ChatGPT

For a custom connector's tools to be available in ChatGPT, several key requirements and conventions must be met:

- **Manifest Compliance:** The MCP server must host a manifest at /.well-known/mcp/manifest.json describing its tools and metadata. ChatGPT fetches this during connector setup. If the manifest is missing required fields or has syntax errors, the connector won't load. In stelae's case, the manifest is properly structured (we verified it contains the server info and a list of 11 tools, each with JSON schema definitions for inputs/outputs). An important part of compliance is including the fundamental search and fetch tools as noted. OpenAI's spec explicitly _mandates_ those two actions [2]. Many off-the-shelf MCP servers (including open-source connectors for services) had to implement stub search/fetch actions to satisfy this. If you don't include them, ChatGPT will throw an error and refuse the connector (as we saw). This was a **common misconfiguration** early on - developers would create a custom toolset but omit a generic search, leading to the "search action not found" error until they added it [2].
- **Transport and Endpoint:** The connector needs to use a supported transport for streaming results. ChatGPT supports **Server-Sent Events (SSE)** and **streaming HTTP** for MCP connectors [5]. SSE is the recommended transport for real-time streaming of tool responses. In practice, many MCP frameworks (like FastMCP in Python) make it easy to spin up an SSE endpoint. If a server only implemented a different transport (e.g. pure WebSocket or non-streaming HTTP without chunking), ChatGPT's connector client might not work with it. In our stelae setup, the mcp-proxy aggregates multiple stdio servers and exposes them via an HTTP/SSE interface [18] [19]. This means the stelae connector endpoint (<https://mcp.infotopology.xyz/stream>) is an SSE stream that ChatGPT can subscribe to. A misconfiguration here could be an incorrect URL or not enabling CORS/HTTPS as needed - if ChatGPT can't reach the endpoint, the connector will fail silently. (Since our attempt yielded no result rather than an explicit error, connectivity might not be the issue; it's more likely the tools were filtered out due to mode and approval policies.)
- **Tool Naming and JSON Schema:** Each tool should be named descriptively, but also conform to ChatGPT's expectations. As noted, **reserved names** like search and fetch have special meaning. Another potential pitfall is if you name a tool something that conflicts with built-in actions or don't make its purpose clear. In stelae's manifest, for example, there is a tool named grep which functions like a code search. In Developer Mode, we could manually invoke grep, but ChatGPT's autonomous deep research likely wouldn't know to use a tool called "grep" unless specifically prompted - it expects a search tool to perform general queries. Proper JSON schema definitions for inputs/outputs are also important so that the assistant knows how to format calls and interpret results [1]. If the schema is incorrect (e.g. wrong types or missing required fields), the model might fail to call the tool or parse its response. We should ensure the search tool returns a JSON with a list of "results" (each containing an id, title, text, etc. as per spec) that the assistant can iterate over [1]. Similarly, fetch should accept an id and return the content of a document. Deviating from these conventions can cause the AI to ignore the results. For example, one developer observed that their deep research agent wasn't using the search results, which could happen if the JSON keys didn't match what ChatGPT expects (like using items instead of results, etc.).
- **Tool Annotations and Permissions:** Each tool in the manifest can include annotations such as readOnly, destructive, and idempotent flags, as well as an "approval" setting. These influence how ChatGPT treats the tool. **Read-only tools** (those that only retrieve data and don't change anything) should be marked "readOnly": true and "destructive": false in their annotations, and typically do not require user confirmation to run [5].
In the stelae manifest, tools like list_allowed_directories and directory_tree have readOnly: true and were given an "approval": "never" setting - meaning the server indicates they never need explicit user approval for safety. Write or exec-style tools (like edit_file or execute_command) are marked readOnly: false (and in some cases destructive: true if they can alter state significantly), and these we configured with "approval": "always", meaning ChatGPT should **always ask the user** before running them. For instance, the execute_command tool in stelae is categorized as an exec action and has approval "always" [2]. If these annotations are mis-set, it can cause confusion or unintended blocks. Imagine marking a dangerous operation as readOnly - the assistant might run it without confirmation, which would be a serious safety issue. Conversely, marking a harmless read action as requiring approval could prevent the AI from using it autonomously in deep research mode.
**Approval gating** in ChatGPT works such that any tool labeled as requiring approval will not be invoked unless the user explicitly authorizes it. In Developer Mode chats, the UI may prompt "The assistant wants to do X, allow?" for such cases. But in Deep Research, there is no user intervention mid-process; thus, any tool that isn't auto-approved will simply be skipped. This is likely why by default ChatGPT's deep research will only use search/fetch (which are implicitly read-only and safe). Ensuring the connector's manifest has appropriate defaults (e.g. a policy section setting read_default to "auto" or "never" require approval for read ops) is critical.
The stelae manifest's policy indicates read_default: "never" and write_default: "always", which we interpret as never prompt for read actions, always prompt for write [2]. If such policy was not respected due to being in the wrong mode, the effect would be that no read tools were called at all (since Deep Research wouldn't stop to prompt for them). This interplay is subtle but a common source of confusion.
- **Workspace and Access Constraints:** Connectors often are configured to only allow access to certain file system paths or resources. For example, the stelae connector's Filesystem MCP is rooted at the project directory. If a tool request goes outside the allowed scope, the server might refuse or return an error. ChatGPT doesn't necessarily know these details, so from its perspective the tool just "fails." In diagnosing issues, we should confirm that the list_allowed_directories tool is indeed supposed to return something (e.g. the root paths allowed). If it required an argument (though in the manifest it takes no input), providing incorrect input could also cause a no-op. In our attempt, we called it with no arguments as intended, so input wasn't the problem - the call simply never happened due to higher-level integration issues.

In summary, to have your MCP connector's tools show up and be callable in ChatGPT, you **must follow the spec** (include search/fetch), use a supported transport (SSE/streaming HTTP), accurately describe each tool's behavior in the manifest, and align the approval settings with how you plan to use it (auto-approved for read tools if you want ChatGPT to use them without asking each time). Deep Research mode will inherently filter to safe read-only tools, whereas Developer Mode will list everything (with warnings or prompts for dangerous ones).

## Developer Mode: How Tools Are Exposed and Used

Enabling Developer Mode in ChatGPT unlocks the full potential of custom connectors. Once you have turned on Developer Mode and added the connector in settings, here's what happens:

- **Tool Listing:** ChatGPT fetches the connector's manifest (making an HTTP GET to the manifest URL provided). If successful, it will list all the tools defined by that connector. In the UI, there is typically a "Tools" panel or a toggle where you can see the available actions. The interface actually allows you to **manage the tools** - for each connector, you can **toggle individual tools on or off**, and you can **refresh** the connector if you've updated the server's manifest [5]. This is useful in development: if you add a new tool or change a description, you don't need to remove/re-add the connector; clicking refresh will pull the latest tool list and schemas from the MCP server [5](https://www.reddit.com/r/MCPservers/comments/1ndqn3f/chatgpt_added_full_support_for_mcp_tools_finally/#:~:text=Manage%20tools%3A). (One common mistake is forgetting to refresh after changing the server - the UI might be working with an outdated cached manifest. During troubleshooting, it's worth hitting the refresh to ensure you're seeing current data, especially if you encounter a "tool not found" or similar issue after deploying changes.)
- **Selecting Connectors in a Conversation:** In Developer Mode, you can choose which connectors to use in a given chat. This is analogous to selecting plugins or data sources. According to OpenAI's notes, you would _"choose Developer mode from the model selector and select connectors"_ before starting your conversation [5]. So, if multiple connectors are added, you can activate one or many. If our stelae connector was properly added, we would need to make sure it's selected for the conversation; otherwise, none of its tools will be considered by the model. Not selecting the connector is another benign misstep that can lead to "why isn't my tool being used" - it sounds obvious, but in a multi-connector environment you have to explicitly enable each source per chat.
- **Invoking Tools via Natural Language:** In Developer Mode conversations, you generally do not call api_tool.call_tool explicitly as a user. Instead, you **prompt** ChatGPT in a way that triggers the model to use the tool. The model is aware of the available tool names, their descriptions, and what inputs they require (from the manifest). It will decide when to call them based on the conversation. For example, if you ask "What files are present in the project directory?", the model might decide this is a good time to call the list_allowed_directories or directory_tree tool from the stelae connector to gather information, and then it will present the results. Sometimes it requires some coaxing - **prompt engineering** can be needed to get the model to pick the right tool. As OpenAI notes, _"you may need to explore different prompting techniques to call the correct tools."_ [5] For instance, including the phrase "using the stelae connector" or directly mentioning the tool's name in your request could encourage the model to use it. Tools marked as readOnly (the manifest's readOnlyHint) are treated as information-gathering steps and the model is generally willing to use them to formulate a response [5]. Tools without that hint (i.e. write actions) are handled more cautiously: the model might only execute them if the user explicitly asks for that action or as a final step after confirmation. In Developer Mode, the assistant might respond with a system message like "I can do X, would you like me to proceed?" for a potentially destructive action.
- **Execution Flow:** When the model calls a tool, under the hood ChatGPT sends a request to the connector's endpoint. For remote connectors, this is done via the cloud (ChatGPT servers reaching out to the provided URL). The call is packaged as a JSON payload including the tool name and arguments, and the connector returns its result (or streams it if using SSE). If the tool returns a large output (say, reading a file or a directory tree), ChatGPT will receive that and then incorporate it into the assistant's answer. In the answer you see, typically the assistant will summarize or quote the result, and often provide a citation link that points back to the connector (for deep research style answers). For example, if directory_tree returned a JSON structure of files, the assistant might summarize it in text and cite the action as a source.
- **Error Handling:** If a tool call fails (due to a server error, timeout, or schema mismatch), ChatGPT's response might contain an apology or an error message, or it may simply not yield any content from that tool. In Developer Mode, the assistant might actually report the error text if the connector provides it. During development, it's wise to check your MCP server logs. If the assistant isn't responding with tool data, see if the server received the request at all. In our case, we suspect the server was never contacted (no list_allowed_directories call made) because the connector hadn't been successfully activated in the session. If the server log shows no activity, the issue is on the ChatGPT side (loading/recognition). If the log shows an incoming call and perhaps an exception, then the issue is within the tool implementation. For debugging, you can also use tools like curl or a web browser to hit the connector's endpoints (/tools/list, /tools/call) to verify they respond as expected. The stelae README, for instance, suggests using curl with jq to inspect the manifest and tool list to ensure the connector is live. Doing this outside of ChatGPT can confirm whether your connector is up and serving the correct data.
- **Security and Approval in Developer Mode:** As mentioned, Developer Mode will enforce any approval flags by pausing for user confirmation. For example, if we tried to use the execute_command tool (with approval "always"), ChatGPT would likely not run it immediately. Instead, it might say something like: "The assistant can execute a command on the server. Do you want to allow this?" Only after the user allows would it proceed. This mechanism prevents accidental destructive actions. A common configuration mistake is not realizing that certain tools are defaulting to require approval. If you find that the assistant says it "cannot do that action" or it simply doesn't attempt a tool even in Developer Mode, check if the manifest's approval setting is forcing a user gate. For testing, you might temporarily set a tool's approval to "never" (no approval needed) to see if it runs, but **use caution**: doing so on a write action can be dangerous, and the OpenAI platform might still refuse truly destructive actions without user consent. Always adhere to safety guidelines.

In our scenario, since we did not have Developer Mode enabled in the environment (it appears we were in Deep Research mode only), ChatGPT didn't surface the stelae tools at all. Had Developer Mode been available, the correct approach would have been: enable it, add the stelae connector (pointing to mcp.infotopology.xyz which serves the manifest), refresh to load all tools, then ask ChatGPT to list directories or perform similar tasks. The expectation is that under those conditions, list_allowed_directories would succeed and return the configured workspace root (e.g. /home/ubuntu/dev/phoenix as per the manifest policy) and any subdirectories allowed. Following that, we could call directory_tree on that path to get a JSON tree of files, and so on for each tool - systematically verifying each tool's behavior in Developer Mode.

## Common Misconfigurations and Issues with MCP Connectors

Setting up custom connectors is powerful but can be tricky. Below we outline some frequent issues (and their root causes) that developers encounter, along with our observations from the stelae case:

- **Missing Required Actions (search/fetch):** As emphasized, not providing a proper search and fetch is the number one integration problem. ChatGPT will outright reject the connector in many cases, showing a _"search action not found"_ error banner during connector setup [2]. The Monday.com connector issue is a prime example - their MCP exposed domain-specific queries but no generic search, causing ChatGPT to fail the handshake [2]. The solution is usually to add a simple search that returns some results (even if it's just dummy or wraps another function) to satisfy the spec [2]. In fact, OpenAI's own examples and community gists explicitly mention _"Without these, ChatGPT will show 'search action not found' error."_ [7]. If you see this error, double-check your /tools/list JSON for the presence of both tools.
- **Tool Name Mismatch or Extra Tools in Non-Dev Mode:** Related to the above, sometimes the tools are present but named incorrectly. For example, an action named find or query might semantically be the search, but ChatGPT is literal in expecting the name **search**. Similarly, you might have multiple search-like tools (e.g., one for files, one for database); however, ChatGPT likely only calls the one named search in deep research. Other custom tools beyond fetch/search will be **ignored in Chat mode**. This "stripping" of the tool list means even though your manifest shows 10 tools, the model in Chat/Research might behave as if only 2 exist. This isn't exactly an error, but it can confuse developers. It confused us initially when none of the 11 stelae tools responded in Deep Research; in reality, the model wasn't allowed to use 9 of them and didn't recognize the remaining 2 because of naming. The fix here is to use Developer Mode for full tool access, and ensure your primary info-retrieval tools are indeed called search/fetch as required.
- **Transport/Connectivity Issues:** Another common problem is the connector server not being reachable. If you run an MCP server on localhost, ChatGPT won't reach it unless you expose it to the internet (or use something like cloudflared as stelae does). We set up stelae with a Cloudflare tunnel and a public URL, which is ideal. However, any network misconfiguration - wrong URL in the connector settings, expired domain, firewall blocking ChatGPT - will result in silent failures. ChatGPT might try to fetch the manifest and time out. In the ChatGPT UI, this could manifest as the connector not appearing after you add it, or an "error fetching connectors" message. Checking the connector's health endpoints (like the manifest URL) from an external perspective can verify connectivity. Ensure you use **HTTPS** for the connector URL; ChatGPT likely requires a secure connection (wss or https). The Monday forum discussion hints that their connector passed Claude but not ChatGPT - Claude might have allowed an insecure connection or bypassed certain checks that OpenAI did not [2]. So always stick to the official guidelines for transports (enable SSE with proper headers, etc.). If using streaming HTTP, make sure to flush data periodically so the assistant receives incremental output.
- **Authentication and OAuth Hurdles:** Custom connectors can be configured with **no auth** or with OAuth (for cases where the connector accesses a third-party API on behalf of the user). If you set up OAuth, there is an additional complexity: the connector's manifest must include an OAuth flow specification, and ChatGPT will require the user to authenticate during connector addition. A misconfiguration in OAuth (wrong client ID/secret, incorrect redirect URI, etc.) can prevent the tools list from loading. In some community reports, an OAuth-protected connector wouldn't retrieve the tools until the user completed auth, and any hiccup there looked like a failure to load. One workaround during development is to start with auth disabled (open data) just to get the connector working, then add auth once the tools themselves function. The help article on connectors notes that even "no authentication" connectors require the user to go through a connect flow in settings (essentially acknowledging the connector) [3]. If a teammate can use the connector but you cannot, check if you have **connected it in your own settings** - each user must authorize a connector for their account, even if an admin added it to a workspace [3]. This could explain a scenario where the connector seems installed but "tools not found" for a particular user.
- **Manifest Content Errors:** Simple JSON mistakes or unsupported features can also cause issues. For example, including certain characters in tool descriptions that ChatGPT's parser can't handle, or using a \$ref in JSON schema that isn't resolved. The safest approach is to test your manifest with the OpenAI platform's own validator if available. Sometimes error messages might appear in the browser console (for the ChatGPT web app) if something fails to parse. Not all errors bubble up to the UI. If the connector doesn't show up at all, try validating the JSON and ensure the content-type served is application/json. In development, one might use the OpenAI _1MCP_ CLI tool (One MCP agent) to check capability lookups; stelae's stack actually runs a 1mcp agent for discovery [19]. While that's more advanced, the point is to use available tools to sanity-check the manifest.
- **Tool Behavior and Result Formatting:** Let's say your connector is loaded and ChatGPT is calling the tools, but the answers are incorrect or incomplete. The issue might be with **what your tools return and how the AI interprets it**. For instance, if your search returns 100 results with lots of text, the model might overwhelm its context window or pick only the first few. Best practice is to limit search results to a reasonable number (the spec doesn't require a particular number, but some examples show maybe 5-10 results). Ensure fetch returns concise text for the document if possible (or chunk it). If your fetch returns an entire PDF's text, the model might truncate it. A "deep research" MCP often uses vector embeddings to do smarter retrieval; however, implementing that is beyond the simple spec requirement. At minimum, verify the result keys: each search result should probably have title and text for the model to quote or cite. If you see the model call search and then immediately call fetch on some id, that's good - it means it parsed the results. If it calls search and then… nothing, perhaps it didn't understand the results or didn't think them useful. Logging the conversation can help here (in Developer Mode, you might even see the intermediate tool outputs as part of the chat). Some community agents recommended formatting the text in search results as a summary or snippet rather than full content, to entice the model to fetch the full doc via the fetch tool when needed.
- **Approval and Safety Settings:** As touched on before, misconfigured approval settings can lead to tools being unavailable. If you accidentally mark a read tool as needing approval, the model won't use it automatically in deep research - you'd have to explicitly instruct it in developer mode and then approve. Similarly, if a write tool is marked as not needing approval ("approval": "never") but actually makes system changes, ChatGPT's safety system might still intervene or at least flag the conversation. OpenAI likely has internal safeguards such that certain categories of actions (file writes, command exec) always require user confirmation even if the manifest says otherwise. From a backend perspective, **ensure your connector categorizes tools appropriately** (e.g., category "fs-read" vs "fs-write" vs "exec"). The stelae manifest uses categories like fs-read for file reads, which is informational, versus exec for the shell tool, etc. These categories aren't formally enforced by the protocol but serve as documentation and hints. If something isn't working, double-check you didn't accidentally label a tool in a way that hides it. For example, one might set a tool's category to "internal" or something not recognized - while it shouldn't matter to the JSON parser, the assistant's prompting might ignore tools that sound like internal/debug.
- **Refreshing and Sync Issues:** As mentioned, always refresh the connector after making changes. Another subtle issue can occur if you run multiple MCP servers under one proxy. The stelae setup combines filesystem, shell, search, etc., under a single proxy endpoint (with a unified manifest). If one of the sub-tools fails to start or register, it might be missing from the manifest. For instance, if the ripgrep process (grep tool) wasn't running, the manifest might exclude it. Then ChatGPT obviously can't call it. So ensure all sub-services are up before testing. Checking curl <http://localhost:9090/tools/list> on the server side (if you have access) can show what tools the proxy is currently exposing. The stelae validation checklist explicitly says to use that endpoint to confirm that search and fetch are published. This ties back to our case: if we had run that and found no search in the list, it would explain everything. The log snippet in README suggests search should be there (via the search shim) - if it wasn't, that's the root cause.

## Testing and Debugging MCP Connectors

When tool calls aren't working, it's important to systematically test the connector both **from within ChatGPT and externally**. Here's a plan from a backend developer's perspective:

- **Verify Manifest Accessibility:** Use a simple HTTP client (curl, Postman, etc.) to GET the manifest URL (e.g. <https://your-domain/.well-known/mcp/manifest.json>). Check that you get a 200 OK and the JSON contains what you expect (especially the tools list). If you get an error or nothing, fix that first (maybe your server isn't running or DNS is wrong). For stelae's public URL, one would do: curl -s <https://mcp.infotopology.xyz/.well-known/mcp/manifest.json> | jq . and look at the output. Also try the /tools/list endpoint (usually BASE_URL/tools/list) which returns just the tools array - sometimes that endpoint may be accessible even if manifest has issues. This ensures the **connector is reachable**.
- **Add Connector in ChatGPT and Observe:** In the ChatGPT UI, go to Settings → Connectors and add the custom connector by providing the URL. If an error like "search action not found" appears immediately, you know what to fix (implement the missing tool) [2]. If it adds successfully, proceed. Then open a new chat, select Developer Mode and **enable the connector** for that chat. If using Deep Research, select it as a source in the "Use connectors" interface. Now, ask ChatGPT a question that should trigger the connector. In deep research, you might ask something that definitely requires internal knowledge (so it has to use the connector's search). In developer mode, you can be more direct: e.g. "List the directories available in the project (use the stelae tools)." If ChatGPT responds with confusion or doesn't use the tool, it might not be seeing the tools. In Developer Mode, you should be able to click a "Tools" icon or see if the assistant made a tool call (the UI sometimes shows an animated "Using &lt;tool&gt;…" message when it does). Lack of that feedback means no attempt was made.
- **Check Connector Logs:** On the MCP server side, ensure you're logging requests. For SSE servers, each tool invocation might be logged to stdout. For example, FastMCP and TBX's mcp-proxy usually log when a tool is called. If you see nothing in the logs after initiating a query from ChatGPT, the request never reached your server - indicating an upstream issue (possibly filtering or not actually selecting the connector as source). If you do see logs of a tool being called, note which tool and what parameters. You can often see the request payload. This helps verify that ChatGPT indeed called (for example) search with the user's query. If the logs show it returned results, but ChatGPT didn't follow up with fetch or didn't output anything, then the problem is likely with **how the AI handled the results**. In such a case, examine the format of your search results JSON. Compare it with OpenAI's examples (should be an object with a "results" key containing a list of items, each with at least an "id" and some text/title) [1]. Perhaps ChatGPT didn't understand the response, so it stopped. Tweaking the format or content (e.g., ensure the text snippet is relevant) could help. It might also be that the query didn't find anything, and the agent gave up - so test with a known query that should find results.
- **Simulate via API Agent:** For deeper debugging, you can remove ChatGPT's interface from the equation and use the API. OpenAI's platform documentation on **"Building MCP servers for ChatGPT and API"** provides guidance [5]. You could use the OpenAI Python SDK or JS SDK to create a function-calling session where the functions correspond to your MCP tools. However, since the MCP protocol is a bit more complex (streaming responses, etc.), the easier path is using the **OpenAI Agents SDK** if you have access. In a minimal scenario, using the hostedMcpTool in a custom agent and calling it with a test prompt can confirm that your server responds correctly [6]. This is essentially how ChatGPT Developer Mode itself is calling your tool behind the scenes. If your agent code can get a correct answer via the connector, but ChatGPT cannot, the issue might be specific to ChatGPT's environment or policies.
- **Refer to Documentation and Forums:** Make use of OpenAI's official docs - there is a **Connectors help article** [3] and the **MCP developer guide** on the platform (which unfortunately requires login). The developer community forum has many threads (some we've cited) where people ask and resolve issues. For example, if OAuth connectors show a 500 error on /list-accessible, the forums suggest re-checking the auth config [8]. If ChatGPT deep research isn't using the returned data, you might find similar experiences shared. As a backend dev, also monitor the OpenAI Developers updates (the OpenAI DevRel team often posts on X/Twitter and the dev forum about new changes or known bugs). For instance, during the rollout, some users reported that deep research sometimes fails to produce a report - which might have been a transient platform issue. It's good to distinguish your connector issues from any platform instability.
- **Iterative Tool Testing:** Once the connector is recognized and basic search/fetch works, test **every tool one by one** in Developer Mode. You can do this by explicitly asking for the tool's function. In our case, we would prompt: "Use list_allowed_directories to show the allowed paths." The model should then call it and return something like a list of directory paths (perhaps just the root path if only one root is allowed). Next: "Use directory_tree on &lt;that_path&gt; with depth 1" to see if it returns a tree JSON (the assistant might summarize it). Then "Use read_text_file to open &lt;somefile&gt;", and so on. By doing this, we can catch if a specific tool is not working. Maybe edit_file might require a special input format (like a diff or new content) which is harder to prompt. If a tool consistently doesn't respond, there could be an implementation bug server-side. Using the connector's API directly can help isolate that (e.g., POST a JSON to /tools/call/edit_file and see if it returns the expected result or error).
- **Safety Review:** Finally, ensure that your tool definitions abide by OpenAI's usage policies. Tools that access sensitive data or perform critical actions might be filtered out or require higher trust levels. For example, a tool that reads emails or database records - even if technically read-only - might be considered sensitive. Currently, ChatGPT doesn't have a fine-grained content inspection for connector outputs (beyond its general content filters), but it's wise to only expose what's necessary. If a tool returns an unexpected type of content that triggers a safety filter, the assistant might refuse to display it (you'd get a message like "I'm sorry, I can't continue with that request"). This is not a "misconfiguration" per se, but something to keep in mind while debugging strange refusals.

By following the above steps, you can usually pinpoint why a connector tool isn't working and address it. In our case with stelae, the diagnosis is that the connector was not active in the current chat (likely due to being in the wrong mode and missing the required search hook). The root cause is a **mode mismatch and spec adherence issue** rather than any bug in the tool itself. The fix would be to add a proper search action to stelae's MCP (if not already present) and use Developer Mode to test the full range of tools.

## Conclusion

Integrating an MCP connector like "stelae" into ChatGPT involves ensuring the connector meets the platform's requirements and using the appropriate mode for the desired functionality. In summary, **ChatGPT's Deep Research and normal modes are limited to read-only operations (search and fetch), whereas Developer Mode is needed to unlock custom read/write tools** [1] [4]. If a tool call isn't going through, first verify that the connector was added properly and that the manifest includes the mandatory actions [2]. Then check for common misconfigurations: naming, transport connectivity, and approval settings being chief among them. We saw that failing to include a search tool will outright block the connector [2], and that any tool requiring user approval won't run in autonomous modes. Developer Mode provides a sandbox to invoke every tool (with user oversight for writes) - it should be used to iteratively test each capability of the MCP server.

From a backend developer's perspective, the MCP integration in ChatGPT is powerful but requires careful setup. The introduction of full MCP support (including write actions) in late 2025 has opened up new possibilities [4], effectively turning ChatGPT into an _"orchestration layer for real work"_ across external systems [4]. With that power comes the need to handle authentication, error handling, and security more rigorously [4]. Common pitfalls like those we diagnosed (spec mismatches, mode restrictions, tool filtering) are usually resolved by closely following OpenAI's guidelines and utilizing the community knowledge base. By systematically testing and debugging - from manifest to model prompts - one can achieve a smooth integration where ChatGPT acts as a natural language interface to your custom tools. In our case, enabling Developer Mode and adjusting the stelae connector to include the expected actions would allow ChatGPT to list directories, search code, execute commands, and more, all from a conversation. As we learned, the keys are **compliance, the right mode, and thorough testing** - with those in place, MCP connectors can significantly extend ChatGPT's capabilities in a reliable way.

### Limitation: Connector-Generated Tool Paths

Developers cannot override the `link_<instance>/...` prefixes that show up in ChatGPT's tool browser (e.g. `/stelae/link_123abc/tools/call`). Those segments are minted by the ChatGPT connector layer when it links a manifest-defined server. Our proxy only exposes a single public facade (`/.well-known/mcp/manifest.json` plus `/mcp`) and then dispatches requests to downstream servers mounted at deterministic names such as `/stelae/fs/…` (see `docs/ARCHITECTURE.md:5-75` and `~/apps/mcp-proxy/http.go:675-754`). Because the hashed link value never appears in our HTTP handlers or logs, there is nothing for us to alias or short-circuit. Likewise, the connector always invokes standard MCP methods (`initialize`, `tools/list`, `tools/call`) rather than a custom "server + action" RPC we could influence. Until OpenAI exposes controls for those link identifiers, the only mitigation is documentation—keep tool names readable and remind users that the opaque prefix is client-generated and may change when the connector is re-linked.

**Sources:**

[1] [ChatGPT Custom MCP Connectors with Developer Mode | by alexeylark | Sep, 2025 | Medium](https://medium.com/@alexeylark/chatgpt-custom-mcp-connectors-with-developer-mode-d791fde17d25)
[2] [ChatGPT MCP connector error - "search action not found," but Claude AI integration works - monday Apps & Developers - monday Community Forum](<https://community.monday.com/t/chatgpt-mcp-connector-error-search-action-not-found-but-claude-ai-integration-works/116466)
[3] [Connectors in ChatGPT | OpenAI Help Center](https://help.openai.com/en/articles/11487775-connectors-in-chatgpt)
[4] [OpenAI Adds Full MCP Tool Support in ChatGPT Developer Mode: Enabling Write Actions, Workflow Automation, and Enterprise Integrations - MarkTechPost](https://www.marktechpost.com/2025/09/11/openai-adds-full-mcp-tool-support-in-chatgpt-developer-mode-enabling-write-actions-workflow-automation-and-enterprise-integrations/)
[5] [ChatGPT added full support for MCP tools ( Finally) : r/MCPservers](https://www.reddit.com/r/MCPservers/comments/1ndqn3f/chatgpt_added_full_support_for_mcp_tools_finally/)
[6] [Model Context Protocol (MCP) | OpenAI Agents SDK](https://openai.github.io/openai-agents-js/guides/mcp/)
[7] [ChatGPT MCP Developer Mode MCP - Complete Tutorial - GitHub Gist](https://gist.github.com/ruvnet/7b6843c457822cbcf42fc4aa635eadbb)
[8] ["Error fetching connectors" & 500 Error from /list-accessible after ...](https://community.openai.com/t/error-fetching-connectors-500-error-from-list-accessible-after-adding-custom-mcp-connector/1319030)
